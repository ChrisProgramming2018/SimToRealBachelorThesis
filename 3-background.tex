\chapter{Background}\label{chap:background}

The explanations of the fundamental elements of reinforcement learning are based on the book Reinforcement Learning:
An Introduction\cite{Sutton1998} from Richard S. Sutton and Andrew G. Barto.

\section{Markov Decision Processes}
The Markov Decision Process (MPD)  is the mathematical framework of Reinforcement Learning and
is defined as a tuple < $\mathcal{S, A, R, T}$ >
\begin{itemize}
\item $\mathcal{S}$ is a set of states, s $\in \mathcal{R}^{n}$
\item $\mathcal{A}$ is a set of actions, a $ \in \mathcal{R}^{n}$
\item $\mathcal{R}$ is the reward
\item $\mathcal{T}$ is the transition probability function
\end{itemize}
Every state in a Markov Decision Process needs to satisfy the Markov Property,
which means that "The future is independent of the past given the present".
The definition is given by a state reward pair 
\begin{equation}
\mathcal{P}(S_{t+1}, R_{t+1} | S_{t}, R_{t}) =  \mathcal{P}(S_{t+1}, R_{t+1} | S_{t}, R_{t}, ..., S_{0}, R_{0})
\end{equation}
Unfortunately for most real-world problems this assumption is violated. This is also the case for the
state transition function, which is defined as
\begin{equation}
  \mathcal{P}(s_{t+1}| s_{t}, a_{t}) =  \mathcal{P}(S_{t+1} = s^{'}| S_{t} = s ,  A_{t} = a)
\end{equation}
 The goal of the agent is to find the policy that maximizes the total reward $(R_{t})$.
In the discounted case is given as 
\begin{equation}
  R_t = \sum^{\infty}_{k = 0} \gamma^k r_{t+k+1}
\end{equation}
with $\gamma$ as the discount factor to exponentially discounts future rewards. 
The policy is a function, which in the deterministic case maps states to actions.
While a stochastic policy $\pi(a|s)$ has a certain probability of selecting an action \textit{a} in state \textit{s}.
For every MDP there is always at least one optimal policy $\pi$. 
In case the transition probability is given, the \textit{dynamic programming} based \textit{value iteration} Algorithm is one way to compute policy $\pi$.
This Algorithm is build upon the concept of a value function.
\subsection{Value Function}
The value function $V^{\pi}$ : $\mathcal{S}$ $\rightarrow$ $\mathcal{R}$, represents the expeceted total reward in a given state when following the policy $\pi$.
It is defined as
 \begin{equation}
  \begin{aligned}
    V^{\pi}(s) &= \mathbb{E}_{\pi} [R_t | s_t = s] \\
    &= \mathbb{E}_{\pi} [\sum^{\infty}_{k=0} \gamma^{k} r_{t+k+1} | s_t = s] \label{MDP:eq1} \\
    &= \sum_{a \in \mathcal{A}} \pi(s, a) \sum_{s' \in \mathcal{S}} \mathcal{T}^{a}_{ss'}[\mathcal{R}^{a}_{ss'} + \gamma V^\pi(s')]
  \end{aligned}
\end{equation}
In each Iteration \textit{i} of the \textit{value iteration} Algorithm the value function is updated by the following equation
\begin{equation}
  V_{i+1}(s) := \max_a \Big\{ \sum_{s', r} P(s',r| s,a) (r + \gamma V_i(s')) \Big\}
\end{equation}
This update is repeated until both sides of the equation are equal.
\subsection{Q-value Function}
In many real-world problems the transition function is unknown and computing the value function is not feasible, because
it would require evaluating all possible actions in a single state at every time step.
To evaluate every action in a state the Q-value Function $ \mathcal{Q}: \mathcal{S} \times \mathcal{A} \rightarrow \mathcal{R} $
\begin{align}
  Q^{\pi}(s, a) &= \textsc{E}_\pi[R_t | s_t = s, a_t = a]  \label{Q:eq1}\\
  &= \textsc{E}_\pi [\sum^\infty_{k=0} \gamma^k r_{t+k+1} | s_t = s, a_t = a]
\end{align}
It represents the expected reward of the agent if the action a is chosen in state s at time step t when following the current policy $\pi$.
The Bellman Optimality Equation is one way to update the Q-value Function 
\begin{equation}
  Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha[r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t)]
\end{equation}
where $ \alpha$ is the \textit{learning rate} and $\gamma$ weights between short and longterm reward.

\subsection{Q-Learning}
Equation (\ref{Q:eq1}) is used to find an optimal policy in the Q-Learning Algorithm represented in Algorithm \ref{alg:q-learning}
\input{figures/background/Q-learning.tex}
The Algorithm is based on the concept of Temporal Difference Learning.
It uses the combined ideas of Dynamic Programming (DP) and Monte Carlo (MC) methods.
Like in Dynamic Programming the old estimate is used to update the new estimate. Unlike MC it uses a bootstraped estimate.

Q-Learning is an off-policy Algorithm, which uses \textit{$\epsilon$  greedy}  policy for interacting with the environment and a \textit{greedy} policy for updating the Q-value Function.

\section{Neural Network}
In complex environments, the state space is continuous which makes it infeasible to store all state-action pairs in a table.
Advances in \textit{machine learning}, especially in \textit{deep learning}, make it possible to approximate the Q-value Function.  
Basic elements of \textit{deep learning} are artificial neural networks, which are inspired by the human brain.
Neural networks have a structure where the \textit{input layer} connected to \textit{hidden layers} followed by the \textit{output layer}. 
A fully connected network has many \textit{hidden layers} represented as nodes. The nodes between different layers are connected by weights. 
The values of the weights can be trained iteratively using an optimization technique like \textit{stochastic gradient descent} and \textit{backpropagation}.
This simple architecture can be seen as matrix multiplications, that makes it a linear function.
By adding nonlinear activation functions it is able to approximate more complex nonlinear functions.
\begin{figure}[h]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/background/nn.png}
  \caption{Deep neural network with several hidden layers}
  \label{fig:tab-training}
\end{figure}



\section{Soft Q-Learning}

Soft Q-Learning uses maximum entropy to augment the standard RL objective.
The agent is additionally rewarded by maximizing the entropy of the policy

\begin{equation}
  \pi^* = \underset{\pi}{\mathrm{argmax}} \sum_{t=0}^{T} E_{(s_t, a_t) \sim \tau_{\pi}}[\gamma^t (r(s_t, a_t) + \alpha \mathcal{H}(\pi(.|s_t))]
\end{equation}

The entropy of the policy $\pi$ is represented by $\mathcal{H}(\pi(.|s_t))]$ and is computed with $-log\pi(.|s_t)$.
This new objectiv leads to a stochastic policy and an implicit exploration.
Parameter $\alpha$ > 0 trades-off between reward and entropy,
but in most cases it is difficult to find a good value.
The authors in Haarnoja et al. 2018 \cite{haarnoja2018soft} 
proposed to adjust $\alpha$ dynamically by taking a gradient step with respect to the loss 

\begin{equation}\label{eq:entropy_temp}
J(\alpha)= \mathbb{E}_{\mathcal{D}, \pi}
\left[
    \log \alpha \cdot  (
        -\log \pi( a_{t} | s_{t} )
        -
        \mathcal{H}_T
    )
\right],
\end{equation}


\section{Actor-Critic Concept}

The Algorithm used in this work is based on the Soft Q-Learning approach and the actor-critic concept.

The \textbf{Actor} is a neural network  approximating our policy function \textit{p(s)} in the continuous action space.  
This is necessary because with infinite actions it is infeasible updating the Q-value Function with the max operater. The most likely way to update the 
Actor weights to enforce the actor to select actions that have a higher Q-value is using the Critic.

A simple way is to minimize the policy gradient loss:

\begin{equation}
  \nabla_{\theta} \mathcal{L}_{\theta}(\mathcal{D}) = - \mathbb{E}_{s \sim \mathcal{D}} [\nabla_{\theta} log \pi_{\theta}(a|s) \cdot Q_{\phi}(s,a)]
\end{equation}


The \textbf{Critic} is used to evaluate the action of the actor. It is a neural network that approximates the Q-value Function and can be trained by minimizing the mean squared error loss
of the training dataset $\mathcal{D}$ represented by the replay buffer

\begin{equation}
  \mathcal{L}_{\phi}(\mathcal{D}) =  \mathbb{E}_{s,a, r, s^{\prime} \sim \mathcal{D}} [Q_{\phi}(s,a) - \underbrace{( r + \gamma \: \underset{a^{\prime}}{max} \:  Q_{\phi^{\prime}}(s^{\prime},a^{\prime})}_{TD - target}]^{2}
\end{equation}
